# -*- coding: utf-8 -*-
"""
Created on Fri Sep 29 16:42:46 2017

@author: admin
"""
# Change Workdir
import os
Workdir = "D:/AlanTan/CNN"
os.chdir(Workdir)

import scipy
import numpy as np
import pandas as pd
from keras.models import Sequential
from keras.layers import Dense, Dropout, Flatten
from keras.layers import Conv2D, MaxPooling2D
from keras.optimizers import SGD
from keras.optimizers import rmsprop
from keras.utils.np_utils import to_categorical
from keras.preprocessing.image import ImageDataGenerator
from keras import backend as k

# load training data from file
TrainData=np.load('TrainData.npz')
x_train = TrainData['TrainImages']
x_train = x_train.astype('float32')
x_train /= 255
TrainImagesName = TrainData['TrainImagesName']
y_train = TrainData['TrainLabels']
# Converting the target variable to the required size 
# from keras.utils.np_utils import to_categorical
y_train = to_categorical(y_train)

# load testing data from file
TestData=np.load('TestData.npz')
x_test = TestData['TestImages']
x_test = x_test.astype('float32')
x_test /= 255
TestImagesName = TestData['TestImagesName']
y_test = TestData['TestLabels']

y_test = to_categorical(y_test)

# Defining the hyperparameters
TransformTrainPath="D:/kaggle/detection/TransformTrain"
TransformTestPath="D:/kaggle/detection/TransformTest"
filters = 32
filtersize = (3,3)
data_augmentation = True
epochs = 5
batchsize = 32
input_shape = (256,256,3)

# Defining the model

model = Sequential()
# input: 100x100 images with 3 channels -> (100, 100, 3) tensors.
# this applies 32 convolution filters of size 3x3 each.
model.add(Conv2D(64, filtersize, activation='relu', input_shape=input_shape))
model.add(Conv2D(64, filtersize, activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.25))

model.add(Conv2D(64, filtersize, activation='relu'))
model.add(Conv2D(64, filtersize, activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.25))

model.add(Flatten())
model.add(Dense(512, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(5, activation='softmax'))

# sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)
rmsprop = rmsprop(lr=0.001)
model.compile(loss='categorical_crossentropy', 
              optimizer=rmsprop, 
              metrics=['accuracy'])

# reset graph if having previous traing 
k.clear_session()

# Starting training
# divided into two kinds of augmentation data

if not data_augmentation:
    print("Not using data augmentation.")
    model.fit(x_train, y_train, 
          batch_size=batchsize, 
          epochs=epochs, 
          validation_split=0.3)
else:
    print("Using real-time data augmentation.")
    # This will do preprocessing and realtime data augmentation:
    train_datagen = ImageDataGenerator(
            zoom_range=0.1,
            rescale=1./255,
            rotation_range=180,
            width_shift_range=0.4,
            height_shift_range=0.4,
            horizontal_flip=True)
    
    test_datagen = ImageDataGenerator(rescale=1./255)
    
    train_generator = train_datagen.flow_from_directory(
            TransformTrainPath,
            batch_size=batchsize)
    
    validation_generator = test_datagen.flow_from_directory(
            TransformTestPath,
            batch_size=batchsize)
    # Compute quantities required for feature-wise normalization
    # if necessary, datagen.fit(x_train)
    
    # Fit the model on the batches generated by datagen.flow_directory().
    model.fit_generator(train_generator,
                        steps_per_epoch=1600,
                        epochs=epochs,
                        validation_data=validation_generator,
                        validation_steps=1000,
                        workers=8)    

# generate predictions on new data
def predict_onepicture_class(model,path):
    oneimage = scipy.misc.imread(path)
    onelabel = model.predict(np.array([oneimage])).argmax() # max value index
    return onelabel

def predict_test_label(model,path):
    p_index = []
    p_real_class = []
    p_predict_class = []
    
    labels = os.listdir(path)
    for i in labels:        
        oneclasspath = os.path.join(path,i)
        files = os.listdir(oneclasspath)
        for j in files:
            p_index.append(j.split(".")[0])
            p_real_class.append(i)
            onefilepath = os.path.join(oneclasspath,j)
            onelabel = predict_onepicture_class(model,onefilepath)
            p_predict_class.append(str(onelabel))
    
    data = {'p_index':p_index,
            'p_real_class':p_real_class,
            'p_predict_class':p_predict_class}
    data_pd = pd.DataFrame(data).set_index('p_index')
    return data_pd   
    
testpd = predict_test_label(model,TransformTestPath)
pd.crosstab(testpd.p_predict_class,testpd.p_real_class,margins=True)

  
    
            
            
            
            

